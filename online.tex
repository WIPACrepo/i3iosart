%!TEX TS-program = pdflatex
%!TEX root = i3det-top.tex
%!TEX encoding = UTF-8 Unicode

\section{\label{online}Online Systems}
\textsl{(John K; 12-15 pages)}

The IceCube online systems comprise both the software and hardware 
at the detector site responsible for data acquisition, event selection,
monitoring, and data storage and movement.  As one of the goals of IceCube
operations is to maximize the fraction of time the detector is sensitive
to neutrino interactions (``uptime''), the online systems are modular so
that failures in one 
particular component do not necessarily prevent the continuation of basic
data acquisition. Additionally, all systems are monitored with a combination of
custom-designed and industry-standard tools so that detector operators can
be alerted in case of abnormal conditions.

\subsection{\label{online:dataflow}Data Flow Overview}

The online data flow consists of a number of steps of data reduction and
selection in the progression from photon detection in the glacial ice to
candidate neutrino event selection, along with associated secondary data
streams and monitoring.  An overview of the data flow is shown in
Fig.~\ref{fig:dataflow}.  

Since the majority of photons detected by the DOMs are dark noise, a
first-level \emph{local coincidence} (LC) is formed between neighboring
DOMs deployed along the same cable, using dedicated wire pairs within the
in-ice cable.  DOM-level triggers, or \emph{hits}, with corresponding
neighbor hits are flagged with the LC condition, while hits without the
condition are compressed more aggressively.  The LC time window as well as
the span of neighbor DOMs up and down the cable can both be configured,
with standard settings of a $\pm1 \mu s$ coincidence window and neighbor
span of 2 DOMs. 

All DOM hits are read out to dedicated computers on the surface by the data
acquisition system (DAQ).  The next level of data selection is the
formation of \emph{triggers} by the DAQ system.  LC-flagged hits across the
detector are examined for temporal and in some cases spatial patterns that
suggest a common causal relationship.  A number of different trigger
algorithms run in parallel, described in Sect.~\ref{sect:trigger}.  All hits
(both LC-flagged and non-LC hits) within a window around the trigger are
combined into \emph{events}, the fundamental output of the DAQ, and written
to disk.  The event rate is approximately 2.5 kHz but varies with the
seasonal atmopheric muon flux, and the total DAQ data rate is approximately
1TB/day.   

The DAQ also produces \emph{secondary streams} that include time
calibration, monitoring, and DOM scaler data.  The scaler data, which is
monitoring the noise rate of each DOM in 1.6 ms bins, is used in the
supernova data acquisition system \cite{sndaq} to detect a global rise from
many $O(10)$ MeV neutrino interactions occuring in the ice from a
Galactic core-collapse supernova.  The time calibration and monitoring
streams are used to monitor the health and quality of the data-taking runs.

The raw DAQ event data is then processed further with a number of
\emph{filters} in order to select a subset of events (less than 10\%) to
transfer over satellite to the Northern Hemisphere (see
Sect.~\ref{sect:filter}).  Each 
filter, typically designed to select events useful for a particular physics
analysis, is run 
over all events using a computing cluster in the IceCube Lab.  Because of
limitations both on total computing power and bounds on the processing time
of each event, only fast directional and energy reconstructions are used.
The processing and filtering system is also responsible for applying
up-to-date calibrations to the DAQ data; processed events, even those not
selected by the online filters, are stored locally for archival.

A dedicated system for data movement handles the local archival storage to
tape or disk, as well as the handoff of satellite data (see Sect.~\ref{sec:jade}).
This includes not only primary data streams but also monitoring data,
calibration runs, and other data streams.

[Add experiment control paragraph]

%An overview of the data flow from DOMs to the satellite.  Description of
%architecture and levels of data reduction, starting with a review of LC and
%proceeding to triggers and then filters.  Secondary streams and SNDAQ.
%I3Live, experiment control, and monitoring.

%The division between \textit{triggering} and
%\textit{filtering} is the urgency of the processing.
%Triggers must operate within a bounded time and if not
%data is lost.  Filters operate behind such large buffers
%that this is not a consideration.  Historically this means
%that the triggers have been rather \emph{dumb} but there
%is in principle no ceiling to the trigger complexity.

Figure: data flow diagram indicating major subsystems to be described in
this section: DAQ (incl. SNDAQ), PnF, I3Live, and SPADE/JADE.

\subsection{SPS and SPTS}

South Pole System: breakdown of computing hardware used at the pole between
hubs, DAQ, PnF, other machines, and infrastructure.  Internal network
bandwidth.  Redundancy, system monitoring (Nagios), and paging system.

Brief mention of SPTS as northern test and validation system.  Replay
capabilities.  

\subsection{Data Readout and Timing}
\subsubsection{Communications and Cable Bandwidth}

Description of communications protocol and messaging strategy.  Reference
to RAPCal and how it fits in.  Event compression.

% Q: is LC signaling documented anywhere yet?  Should it go here?

\subsubsection{Master Clock System}

The GPS clock and time string fanout tree, from master clock (and hot
spare), to Tier I and Tier II fanouts, the DSB card, and into the DOR card.

\subsubsection{DOR Card and Driver}

DOR card description: comms / readout, power control and measurement,
RAPCal initiation, and clock string readout.  Clock modes (internal /
external).  DOMs per card and cards per hub.

Brief description of driver.  Proc file interface.  Data transfer over PCI
bus via DMA.  

Figure: Combined clock fanout tree hierarchy and hub diagram (DSB and DOR
cards, power distribution).

\subsection{Processing at the Surface}
\textsl{(Dave G; 2-3 pages)}

Big-picture description of what DAQ does: collecting hits from the DOMs,
triggering on HLC hits, and packaging waveforms into events.  

\subsubsection{DOMHub and Hit Spooling}

Responsibility of StringHub.  Splicer and description of HKN1 algorithm.
Forwarding of HLC hit times to trigger.  Translation of DOR times into
UTC.  Servicing event readout requests (defer discussion to event builder
section).  Generation of secondary streams. 

Hitspooling.  Mention of hit daemon plans.  

\subsubsection{Supernova System}

SN secondary stream from DOMs and SNDAQ reference.  Interface to
hitspooling.  

\subsubsection{Triggers}

General description of trigger architecture.  Separation of trigger window
and readout window.  How trigger windows depend on geometry.  Thorough
escription of all different trigger algorithms. Trigger and readout window
merging. 

% Note: JK VLVnT proceedings may be useful here

Table: standard settings for triggers

Figure: trigger windows and readout windows.

Figure: example bright multi-trigger event.  

Figure: SLOP triplet geometry?

\subsubsection{Event Building}

Readout requests to StringHub components and packaging of waveforms into
events.  Spooling to disk and interface to PnF.

%\subsubsection{The Secondary Streams (SN/Moni/TCAL)}
%Is this necessary or can we just mention these in the StringHub section?

\subsubsection{Configuration}

Tree of XML configuration files for components, triggers, and DOM settings.  

\subsubsection{Distributed Network Control}

CnC server description.  XML-RPC.  % Plus ZMQ?

\subsection{Online Filtering}
\textsl{(Erik B.; 3-4 pages)}
\subsubsection{Overview}
Big picture view of what PnF is and does (route data from DAQ output to
files for pickup by data handling, applying calibration, recos and
filters along the way, TFT content, etc)

\subsubsection{System Design}
Cover the overall system design and tech used (CORBA, I3Inlet/I3Outlets,
etc) and how all pieces fit together and are controlled (mention
pf2live). Words about historical operation in Plan B in early seasons,
plan A.

\subsubsection{Components}
A brief description of each PnF component.
\paragraph{I3DAQDispatch}
\paragraph{PnF Server}
\paragraph{PnF Clients}
\paragraph{DB Cache}
\paragraph{Realtime Followup server and clients}

\subsubsection{Performance}
\paragraph{Overall rates and known limits (server + I/O limits)}
\paragraph{Average system latency}

Figure: plot of typical latency including run transition?

\subsection{Data Handling}
\textsl{(P. Meade; 1 page)}

Generation description of system architecture.  Stream definitions, dropboxes,
and data pickup.  Archival vs. transfer to TDRSS system.  

\subsection{I3Live and Remote Monitoring}
\textsl{(J. Braun; 1 page)}

I3Live duties of experiment control and realtime monitoring.  Component
descriptions.  Failover modes, alarms, and interface to paging system.
Monitoring system and Moni2.0.  Interface to ITS and its replacement
system, I3MS. 

\subsection{Operational Performance}
\textsl{(John K.; 1 page)}

Explanation of how design choices, system monitoring, and winterovers result in
high uptime.  Discussion of median downtime and various causes of downtime.
Possible basic failure analysis of hardware components.  

Figure: DAQ full uptime and clean uptime percentage.

Figure (optional): Downtime histogram.  
