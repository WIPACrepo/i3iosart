%!TEX TS-program = pdflatex
%!TEX root = i3det-top.tex
%!TEX encoding = UTF-8 Unicode


\section{\label{online}Online Systems}
\textsl{(John K; 12-15 pages)}

The IceCube online systems comprise both the software and hardware 
at the detector site responsible for data acquisition, event selection,
monitoring, and data storage and movement.  As one of the goals of IceCube
operations is to maximize the fraction of time the detector is sensitive
to neutrino interactions (``uptime''), the online systems are modular so
that failures in one 
particular component do not necessarily prevent the continuation of basic
data acquisition. Additionally, all systems are monitored with a combination of
custom-designed and industry-standard tools so that detector operators can
be alerted in case of abnormal conditions.

\subsection{\label{online:dataflow}Data Flow Overview}

The online data flow consists of a number of steps of data reduction and
selection in the progression from photon detection in the glacial ice to
candidate neutrino event selection, along with associated secondary data
streams and monitoring.  An overview of the data flow is shown in
Fig.~\ref{fig:dataflow}.  

Since the majority of photons detected by the DOMs are dark noise, a
first-level \emph{local coincidence} (LC) is formed between neighboring
DOMs deployed along the same cable, using dedicated wire pairs within the
in-ice cable.  DOM-level triggers, or \emph{hits}, with corresponding
neighbor hits are flagged with the LC condition, while hits without the
condition are compressed more aggressively.  The LC time window as well as
the span of neighbor DOMs up and down the cable can both be configured,
with standard settings of a $\pm1 \mu s$ coincidence window and neighbor
span of 2 DOMs. 

All DOM hits are read out to dedicated computers on the surface by the data
acquisition system (DAQ).  The next level of data selection is the
formation of \emph{triggers} by the DAQ system.  LC-flagged hits across the
detector are examined for temporal and in some cases spatial patterns that
suggest a common causal relationship.  A number of different trigger
algorithms run in parallel, described in Sect.~\ref{sect:trigger}.  All hits
(both LC-flagged and non-LC hits) within a window around the trigger are
combined into \emph{events}, the fundamental output of the DAQ, and written
to disk.  The event rate is approximately 2.5 kHz but varies with the
seasonal atmopheric muon flux, and the total DAQ data rate is approximately
1TB/day.   

The DAQ also produces \emph{secondary streams} that include time
calibration, monitoring, and DOM scaler data.  The scaler data, which is
monitoring the noise rate of each DOM in 1.6 ms bins, is used in the
supernova data acquisition system \cite{sndaq} to detect a global rise from
many $O(10)$ MeV neutrino interactions occuring in the ice from a
Galactic core-collapse supernova.  The time calibration and monitoring
streams are used to monitor the health and quality of the data-taking runs.

The raw DAQ event data is then processed further with a number of
\emph{filters} in order to select a subset of events (less than 10\%) to
transfer over satellite to the Northern Hemisphere (see
Sect.~\ref{sect:filter}).  Each 
filter, typically designed to select events useful for a particular physics
analysis, is run 
over all events using a computing cluster in the IceCube Lab.  Because of
limitations both on total computing power and bounds on the processing time
of each event, only fast directional and energy reconstructions are used.
The processing and filtering system is also responsible for applying
up-to-date calibrations to the DAQ data; processed events, even those not
selected by the online filters, are stored locally for archival.

A dedicated system for data movement handles the local archival storage to
tape or disk, as well as the handoff of satellite data (see Sect.~\ref{sec:jade}).
This includes not only primary data streams but also monitoring data,
calibration runs, and other data streams.

[Add experiment control paragraph]

%An overview of the data flow from DOMs to the satellite.  Description of
%architecture and levels of data reduction, starting with a review of LC and
%proceeding to triggers and then filters.  Secondary streams and SNDAQ.
%I3Live, experiment control, and monitoring.

%The division between \textit{triggering} and
%\textit{filtering} is the urgency of the processing.
%Triggers must operate within a bounded time and if not
%data is lost.  Filters operate behind such large buffers
%that this is not a consideration.  Historically this means
%that the triggers have been rather \emph{dumb} but there
%is in principle no ceiling to the trigger complexity.

Figure: data flow diagram indicating major subsystems to be described in
this section: DAQ (incl. SNDAQ), PnF, I3Live, and SPADE/JADE.

\subsection{SPS and SPTS}

South Pole System: breakdown of computing hardware used at the pole between
hubs, DAQ, PnF, other machines, and infrastructure.  Internal network
bandwidth.  Redundancy, system monitoring (Nagios), and paging system.

Brief mention of SPTS as northern test and validation system.  Replay
capabilities.  

\subsection{Data Readout and Timing}
\subsubsection{Communications and Cable Bandwidth}

Description of communications protocol and messaging strategy.  Reference
to RAPCal and how it fits in.  Event compression.

% Q: is LC signaling documented anywhere yet?  Should it go here?

\subsubsection{Master Clock System}

The GPS clock and time string fanout tree, from master clock (and hot
spare), to Tier I and Tier II fanouts, the DSB card, and into the DOR card.

\subsubsection{DOR Card and Driver}

DOR card description: comms / readout, power control and measurement,
RAPCal initiation, and clock string readout.  Clock modes (internal /
external).  DOMs per card and cards per hub.

Brief description of driver.  Proc file interface.  Data transfer over PCI
bus via DMA.  

Figure: Combined clock fanout tree hierarchy and hub diagram (DSB and DOR
cards, power distribution).

\subsection{Processing at the Surface}
\textsl{(Dave G; 2-3 pages)}

Big-picture description of what DAQ does: collecting hits from the DOMs,
triggering on HLC hits, and packaging waveforms into events.  

\subsubsection{DOMHub and Hit Spooling}

Responsibility of StringHub.  Splicer and description of HKN1 algorithm.
Forwarding of HLC hit times to trigger.  Translation of DOR times into
UTC.  Servicing event readout requests (defer discussion to event builder
section).  Generation of secondary streams. 

Hitspooling.  Mention of hit daemon plans.  

\subsubsection{Supernova System}

SN secondary stream from DOMs and SNDAQ reference.  Interface to
hitspooling.  

\subsubsection{Triggers}

General description of trigger architecture.  Separation of trigger window
and readout window.  How trigger windows depend on geometry.  Thorough
escription of all different trigger algorithms. Trigger and readout window
merging. 

% Note: JK VLVnT proceedings may be useful here

Table: standard settings for triggers

Figure: trigger windows and readout windows.

Figure: example bright multi-trigger event.  

Figure: SLOP triplet geometry?

\subsubsection{Event Building}

Readout requests to StringHub components and packaging of waveforms into
events.  Spooling to disk and interface to PnF.

%\subsubsection{The Secondary Streams (SN/Moni/TCAL)}
%Is this necessary or can we just mention these in the StringHub section?

\subsubsection{Configuration}

Tree of XML configuration files for components, triggers, and DOM settings.  

\subsubsection{Distributed Network Control}

CnC server description.  XML-RPC.  % Plus ZMQ?

\subsection{Online Filtering}
\textsl{(Erik B.; 3-4 pages)}
\subsubsection{Overview}

The online processing and filtering system is charged with the immediate handling of all triggered events collected by the data
acquisition system and reducing the data volume to a level that can be
accommodated in our satellite bandwidth allocations ($\sim$100 GB/day).
 This treatment includes application of calibration constants, application of event characterization and selections,  
extracting data quality monitoring information, generation of realtime alerts for events of astrophysical interest
and creation of data files and metadata information for long term archiving.  The online processing and filtering system
is a custom software suite that utilizes a computer cluster of $\sim$20 standard servers located in SPS computing cluster.
The online processing and filtering system has been in operation since the
start of operation of the 22 string configuration of IceCube in 2007.

In IceCube, each triggered event consists of a collection of digitized waveforms recorded by the digital optical modules (ref daq-dom paper).
To be useful for physics, each of these waveforms requires application of calibration constants that allow the waveform units
to be converted from the raw units (ADC counts per sample bin) to more physical units (mV measured in each fixed time, 3.3 ns bin).  These
calibration constants are independently measured (domcal ref?) and stored in an online database for use by
the online processing and filtering system.  Next, each DOM's waveform is deconvolved using the known DOM response
to photons to extract the light arrival time and amplitude information.  This series of time and amplitude light arrival information
for each DOM is the base for event reconstruction and characterization.  The online processing and filtering system encodes
this information for each DOM in a compact data format known as the SuperDST, and occupies $\sim$5\%  of the file size
of the full waveform information.  Any DOM readout whose SuperDST information is found not to be a good representation of the
original waveform, or sees very high amounts of light also have the full waveform readout saved as an addition to the SuperDST record.

Each event is then characterized with a series of event reconstruction algorithms that attempt to match
the observed patterns of recorded light in the SuperDST with known patterns of light
from track and showering event hypotheses (ref reco papers?  e-reco paper?).  These characterizations (location, direction, and energy) and their 
overall goodness-of-fit produced by these reconstructions are used to select interesting 
events by a filter selection.  The filter criteria are set by the IceCube collaboration for each season
and are tuned to select events of interest to specific analyses.  Each season there are
about 2 dozen unique filter selections in operation.  Some of these filters trigger are
designed to search for neutrino events of wide astrophysical interest to the scientific community and trigger
alerts that are distributed to followup observatories worldwide. (ref SN-OFU paper, others?)

The online processing and filtering system also extracts and aggregates data quality and monitoring information
from the data as is it processed.  This information includes stability and quality information from the
DOM waveform and calibration process, rates of DOM readouts, and rates and stability
information for all detector triggers and filters.  This information is aggregated for each data segment and
reported to the IceCube Live monitoring system.  

Finally the online processing and filtering system writes several data files that make up the long-term data archive of the IceCube
experiment.  These include:
\begin{itemize}
\item \emph {Filtered data files} These files contain only events selected by the online filter selections.  These events
generally only include the SuperDST version of the DOM information and results from the online event reconstructions.  These
files are queued for transmission to the IceCube data warehouse by the data handling system using the TDRS satellites. 
\item \emph {SuperDST data files} These files contain the SuperDST version of DOM readout information for all triggered events as well as summary
information from the online filtering process.  This file set is intended as the long-term archive version of IceCube data.
\item \emph {Raw data files}  These files contain all uncalibrated waveforms from all DOMs for every event.  This large data
set is saved until final data quality assurance on the SuperDST sample can be completed.
\end{itemize}

During normal operations, the data acquisition system produces a raw data output of $\sim$1 TB of raw data per day, which result in
a raw data file archive of the same size.  The SuperDST  and filtered data archive, after data compression, are $\sim$170 GB/day and $\sim$90 GB/day,
respectively.  (TODO: verify these numbers.)
\subsubsection{System Design}

The online processing and filtering system uses a modular design, where each component
is responsible for a portion of data processing built around a central master server node and a
scalable number of processing client nodes.  The central master server focuses on
data distribution and aggregation tasks (requesting data blocks from the DAQ, collating event SuperDST, reconstruction, filter,
and monitoring information and writing data files), while the client process focus on the per-event
processing tasks (event calibration, reconstruction, analysis, and filtering).  

The system is built upon the IceCube analysis software framework, IceTray (ref?), allowing standard IceCube algorithms to
be used in the online processing and filtering system without modifications.  Additionally, the system uses the 
Common Object Request Broker Architecture (CORBA) system as means for controlling, supervising and interconnecting
the modular portions of the system.  Specialized classes are used to provide CORBA interconnections
within the IceTray system, allowing file-like interfaces that let data to stream from one component to another 
using native IceTray formats.  Use of a CORBA name server and a dynamic architecture allow for the
addition and remove of filtering clients as needed to meet the processing load from annual filtering
changes and overall rate variations from seasonal variations in the detector trigger rate or moving system components within the SPS
computer system as needed.

\subsubsection{Components}
The flow of triggered event data in the online processing and filtering system is shown in Figure~\ref{fig:online_pnf_internals}
highlighting the flow of data from the DAQ system, though the master server and clients, to files on disk and
online alerts.  Several standard components in the online processing and filtering system include:
\begin{itemize}
\item \emph {DAQ Dispatch} is a process to pickup event data from the data acquisition system data cache and forward to
the PFServer components.
\item \emph {Central Server} are central data flow managers within the  online processing and filtering system.  These servers 
receive data from the I3DAQDispatch event source, distribute events to and record results returning from the PFClient farm,
and send events to file, monitoring and alert writing components.  Typically there are 4 servers used
in operation.
\item \emph {Filter Clients} is the core calibration, reconstruction and filtering process that is applied to each triggered event.  
In normal operation, up to 500 of these clients operate in parallel to filter events in real time.
\item \emph {DB Dispatch} is a DB caching system to prevent the ~400 PFClient processes from overwhelming the 
DB system when requesting calibration information.  This system aggregates DB requests, makes a single
database query and shares the results with all PFClients.
\item \emph {File Writers} are responsible for creation of files and meta-data for the IceCube data archive.  These
files are written in standard IceTray file format.  There is one writer component for each file type created.
\item \emph {Online Writer} is responsible for extracting event reconstruction and filter information from the data for events of astrophysical interest 
and sending this information out  in real time via the IceCube Live alert system.
\item \emph {Monitoring Writer} is responsible for aggregating per-event monitoring information, creating histograms
and forwarding them to the IceCube Live monitoring system.  
\item \emph {Filtered Data Dispatch} and FollowUp clients are responsible for looking for bursts of neutrino events on timescales from 100 seconds up to
3 weeks in duration. This post-filtering search of the data has all filter-selected events available, enabling event to event correlations to discovered.  Any 
significant burst of neutrinos found generates alerts sent to partner observatories worldwide.
\end{itemize}

\begin{figure}[!h]
 \centering
 \includegraphics[width=0.8\textwidth]{graphics/online/pnf/PnF_Internals.pdf}
 \caption{Internal components of the Online Processing and Filtering System.  Arrows highlight the flow of data within the system.}
 \label{fig:online_pnf_internals}
\end{figure}

\subsubsection{Performance}
The online processing and filtering system is designed to filter triggered events as quickly as possible after collection by
the data acquisition system.   A key metric is processing system latency, defined as the duration of time between the data acquisition trigger 
and the completion of event processing and filtering.  A typical latency history for the system is shown in Figure~\ref{fig:online_pnf_latency}, showing
typical system latencies of $\sim$20 seconds.

\begin{figure}[!h]
 \centering
 \includegraphics[width=0.8\textwidth]{graphics/online/pnf/pnf_latency.png}
 \caption{Typical Online Processing and Filtering System latency for a several day period.  The latency defined as the time between DAQ event time and time when the
 online filtering processing is complete.  The spikes in latency correspond to DAQ run transitions.}
 \label{fig:online_pnf_latency}
\end{figure}

(Q: include other system performance information?  Performance bottlenecks?)

\subsection{Data Handling}
\textsl{(P. Meade; 1 page)}

\textsl{Generation description of system architecture.  Stream definitions, dropboxes,
and data pickup.  Archival vs. transfer to TDRSS system.  }

Data handling is provided by three servers named jade02, jade03, and jade04. The jade servers operate independently of one another and 
each of them are capable of handling the nominal data volume by itself. Having three servers allows for data handling to continue seamlessly 
in case of hardware failure or maintenance.

Each server runs a copy of the Java Archival and Data Exchange (JADE) software (stylized “jade”). As its name implies, the jade software 
is written in the Java programming language. It is a reimplementation and expansion of earlier prototype software called South Pole Archival
and Data Exchange (SPADE), written by Cindy Mackenzie. The jade software has four primary tasks: consumption, archival, satellite transmission, and real-time
transmission.

The jade software is configured with a number of data streams, which consist of a data server, a dropbox directory, and a filename pattern. 
The data stream dropbox directories are checked on a regular basis for new files. A file pairing scheme (binary and semaphore) prevents files 
from being consumed before they are finished being produced. For each file, a checksum calculated on the data server is compared to a checksum 
calculated on the jade server. This method ensures that the file was copied without error. After this, the original data file is removed from the data host.

After consumption, files are routed according to the configuration of their data stream. Files that are too large to send via the satellite link are archived to
a configurable number of archival media copies. The prototype SPADE software archived to LTO tapes, while the later jade software archives to large (2+ TB) hard
disk drives. All of the archival data is buffered on the jade server until the archival media is complete. In case of failure while creating the archival media, 
all of the files can be immediately written to fresh archival media with a single command.

Files that are too large to send via the real-time link, but small enough to send via the satellite link are queued for satellite transmission. The jade software
attempts to bundle multiple files together into 1 GB bundle archives to allow satellite link operators to manage the daily data transmission. Very large files
(>1 GB) are split apart into multiple 1 GB bundles for the same reason. The jade software will only transfer a configurable number of bundles to the satellite
relay server. If satellite transmission is not possible, the jade software will buffer the excess bundles on the jade server, to avoid flooding the relay server
unnecessarily.


Small files (<50 KB) with high priority status information are sent via the real-time link. The real-time link is provided by the IceCube Messaging Service
(I3MS). The jade software uses JeroMQ, a pure Java implementation of the ZeroMQ (ZMQ) protocol, to connect to I3MS. In cases where the real-time link is not
available, I3MS will queue the messages to be sent when the link becomes available. All I3MS messages are also sent to jade to send via the satellite link to
ensure delivery if the real-time link should be unavailable for an extended period of time.

\subsection{I3Live and Remote Monitoring}
\textsl{(J. Braun; 1 page)}

I3Live duties of experiment control and realtime monitoring.  Component
descriptions.  Failover modes, alarms, and interface to paging system.
Monitoring system and Moni2.0.  Interface to ITS and its replacement
system, I3MS. 

\subsection{Operational Performance}
\textsl{(John K.; 1 page)}

Explanation of how design choices, system monitoring, and winterovers result in
high uptime.  Discussion of median downtime and various causes of downtime.
Possible basic failure analysis of hardware components.  

Figure: DAQ full uptime and clean uptime percentage.

Figure (optional): Downtime histogram.  
