%!TEX TS-program = pdflatex
%!TEX root = i3det-top.tex
%!TEX encoding = UTF-8 Unicode

\section{\label{online}Online Systems}
\textsl{(John K; 12-15 pages)}

The IceCube online systems comprise both the software and hardware 
at the detector site responsible for data acquisition, event selection,
monitoring, and data storage and movement.  As one of the goals of IceCube
operations is to maximize the fraction of time the detector is sensitive
to neutrino interactions (``uptime''), the online systems are modular so
that failures in one 
particular component do not necessarily prevent the continuation of basic
data acquisition. Additionally, all systems are monitored with a combination of
custom-designed and industry-standard tools so that detector operators can
be alerted in case of abnormal conditions.

\subsection{\label{online:dataflow}Data Flow Overview}

The online data flow consists of a number of steps of data reduction and
selection in the progression from photon detection in the glacial ice to
candidate neutrino event selection, along with associated secondary data
streams and monitoring.  An overview of the data flow is shown in
Fig.~\ref{fig:dataflow}.  

Since the majority of photons detected by the DOMs are dark noise, a
first-level \emph{local coincidence} (LC) is formed between neighboring
DOMs deployed along the same cable, using dedicated wire pairs within the
in-ice cable.  DOM-level triggers, or \emph{hits}, with corresponding
neighbor hits are flagged with the LC condition, while hits without the
condition are compressed more aggressively.  The LC time window as well as
the span of neighbor DOMs up and down the cable can both be configured,
with standard settings of a $\pm1 \mu s$ coincidence window and neighbor
span of 2 DOMs. 

All DOM hits are read out to dedicated computers on the surface by the data
acquisition system (DAQ).  The next level of data selection is the
formation of \emph{triggers} by the DAQ system.  LC-flagged hits across the
detector are examined for temporal and in some cases spatial patterns that
suggest a common causal relationship.  A number of different trigger
algorithms run in parallel, described in Sect.~\ref{sect:trigger}.  All hits
(both LC-flagged and non-LC hits) within a window around the trigger are
combined into \emph{events}, the fundamental output of the DAQ, and written
to disk.  The event rate is approximately 2.5 kHz but varies with the
seasonal atmopheric muon flux, and the total DAQ data rate is approximately
1TB/day.   

The DAQ also produces \emph{secondary streams} that include time
calibration, monitoring, and DOM scaler data.  The scaler data, which is
monitoring the noise rate of each DOM in 1.6 ms bins, is used in the
supernova data acquisition system \cite{sndaq} to detect a global rise from
many $O(10)$ MeV neutrino interactions occuring in the ice from a
Galactic core-collapse supernova.  The time calibration and monitoring
streams are used to monitor the health and quality of the data-taking runs.

The raw DAQ event data is then processed further with a number of
\emph{filters} in order to select a subset of events (less than 10\%) to
transfer over satellite to the Northern Hemisphere (see
Sect.~\ref{sect:filter}).  Each 
filter, typically designed to select events useful for a particular physics
analysis, is run 
over all events using a computing cluster in the IceCube Lab.  Because of
limitations both on total computing power and bounds on the processing time
of each event, only fast directional and energy reconstructions are used.
The processing and filtering system is also responsible for applying
up-to-date calibrations to the DAQ data; processed events, even those not
selected by the online filters, are stored locally for archival.

A dedicated system for data movement handles the local archival storage to
tape or disk, as well as the handoff of satellite data (see Sect.~\ref{sec:jade}).
This includes not only primary data streams but also monitoring data,
calibration runs, and other data streams.

[Add experiment control paragraph]

%An overview of the data flow from DOMs to the satellite.  Description of
%architecture and levels of data reduction, starting with a review of LC and
%proceeding to triggers and then filters.  Secondary streams and SNDAQ.
%I3Live, experiment control, and monitoring.

%The division between \textit{triggering} and
%\textit{filtering} is the urgency of the processing.
%Triggers must operate within a bounded time and if not
%data is lost.  Filters operate behind such large buffers
%that this is not a consideration.  Historically this means
%that the triggers have been rather \emph{dumb} but there
%is in principle no ceiling to the trigger complexity.

Figure: data flow diagram indicating major subsystems to be described in
this section: DAQ (incl. SNDAQ), PnF, I3Live, and SPADE/JADE.

\subsection{SPS and SPTS}

South Pole System: breakdown of computing hardware used at the pole between
hubs, DAQ, PnF, other machines, and infrastructure.  Internal network
bandwidth.  Redundancy, system monitoring (Nagios), and paging system.

Brief mention of SPTS as northern test and validation system.  Replay
capabilities.  

\subsection{Data Readout and Timing}
\subsubsection{Communications and Cable Bandwidth}

Description of communications protocol and messaging strategy.  Reference
to RAPCal and how it fits in.  Event compression.

% Q: is LC signaling documented anywhere yet?  Should it go here?

\subsubsection{Master Clock System}

The GPS clock and time string fanout tree, from master clock (and hot
spare), to Tier I and Tier II fanouts, the DSB card, and into the DOR card.

\subsubsection{DOR Card and Driver}

DOR card description: comms / readout, power control and measurement,
RAPCal initiation, and clock string readout.  Clock modes (internal /
external).  DOMs per card and cards per hub.

Brief description of driver.  Proc file interface.  Data transfer over PCI
bus via DMA.  

Figure: Combined clock fanout tree hierarchy and hub diagram (DSB and DOR
cards, power distribution).

\subsection{Processing at the Surface}
\textsl{(Dave G; 2-3 pages)}

IceCube's data acquisition system (DAQ) is a set of components running on
dedicated servers in the IceCube Lab.  As described in
\ref{sect:online:dataflow}, physics data is read from the DOMs by the StringHub
component and a minimal representation of each HLC hit is forwarded to the
``local'' Trigger components.  The ``local'' Trigger components run these
minimal hits through a configurable set of algorithms and form windows around
interesting temporal and/or spatial patterns.  These time windows are collected
by the ``global'' Trigger and used to form non-overlapping trigger requests.
These trigger requests are used by the Event Builder component as templates to
gather the complete hit data from each StringHub and assemble the final events.

\subsubsection{DOMHub and Hit Spooling}

%Responsibility of StringHub.
The \emph{StringHub} is responsible for periodically reading all available data
from each of its connected DOMs and passing that data onto the downstream
consumers.  It also saves all hits to a local ``hit spool'' disk cache, as
well as queuing them in an in-memory cache to service future requests from
the \emph{Event Builder} for full waveform data.

%Division of front end (Omicron) and back end (Sender).
The StringHub component is divided into two logical pieces.  The front-end
\emph{Omicron} controls all of the connected DOMs, forwarding any
non-physics data to its downstream consumers and sorting the hits using
an HKN1 tree before passing them to the back-end \emph{Sender}.  The Sender
caches SLC and HLC hits in memory, then forwards a few fields from each HLC
hit to the local Triggers.

%Servicing event readout requests (defer discussion to event builder section).
After the
Trigger components have determined interesting time intervals, the Event
Builder will send each interval to the Sender, and it will return a list of
all hits within the interval, pruning the in-memory hit cache of all older hits
after each interval.

%Forwarding of HLC hit times to trigger.
In order to avoid saturating the 2006-era network, the StringHub sends only
simplified HLC hit data to the local Triggers.  Fields extracted from the
full HLC are the string and DOM, the time of the hit, and a few DOM trigger
fields (type, mode, configuration ID), for a total of 38 bytes per hit.


%Translation of DOR times into UTC.
\textsl{Dave G doesn't know the details of this}

%Splicer and description of HKN1 algorithm.
One core assumption of the DAQ is that each component operates on a
time-ordered stream of data.  IceCube's DAQ uses its \emph{Splicer} to
accomplish this.  The Splicer is a common object which gathers all input
streams during the setup phase; no inputs can be added once it's started.  Each
stream pushes new data onto a ``tail'' and the Splicer uses the HKN1 algorithm
to collate the data from all streams into a single output stream.  When a
stream is closed, it pushes an end-of-stream marker onto its tail which causes
the Splicer to ignore all further data.

The HKN1 (Hansen-Krasberg-1) algorithm at the core of the Splicer uses a tree
of \emph{Nodes} to sort a constant number of time-ordered data streams.  Each
Node contains a queue of pending data, a handle for a peer Node, and another
handle for a sink Node (shared with the peer) to which sorted data is sent.

The tree is assembled by associating each input stream with a separate Node
and adding the Node to a list.  This list is then run through a loop which
pulls off the first two Nodes, links them together as peers and creates a
new Node which will act as the sink for both Nodes.  The new Node is then added
to the list.  The loop exits when there is only one Node left.

When a new piece of data arrives, it is added to the associated Node's queue.
That Node then goes into a loop where it compares the top item from its queue
with the top item from the peer's queue and the older of the two items is passed
onto the sink Node (which adds the item to its queue and starts its own
comparison loop.)  This loop stops as soon as the original Node or its peer
has no more data.

%Generation of secondary streams.
%Most of this is stolen from the Overview section above
%I'm not sure what else is needed here.
Along with physics data, DOMs produce three additional streams of data.
The scaler data, which is monitoring the noise rate of each DOM in 1.6 ms bins,
is used in the supernova data acquisition system \cite{sndaq} to detect a
global rise from many $O(10)$ MeV neutrino interactions occuring in the ice
from a Galactic core-collapse supernova.  The time calibration and monitoring
streams are used to monitor the health and quality of the data-taking runs.

%Hitspooling.
As hits move from the front end to the back end, they are written to the
``hit spool'', a disk-based cache of files containing hits.  These files are
written in a circular order so that the newest hits overwrite the oldest data,
and the first hit time for each file is stored in an SQLite database.

Subsystems such as SnDAQ and HESE can send time interval requests to a HitSpool
master daemon which then gathers the hits in that interval from each hub and
forwards them to a final ``sender'' daemon which bundles them up and submits
them to JADE for transfer to the North where the full waveforms can be used for
further analysis.

%Mention of hit daemon plans.
One problem with the current DAQ design is that it only reads data when the
DAQ is running, so the detector is essentially ``off'' during hardware failures
or the periodic full restarts of the system.  There is a plan to split the
StringHub into several independent pieces to eliminate these blind spots.
The front end (Omicron) piece will be moved to an always-on daemon which
continuously writes data (including secondary, non-physics data and other
metadata) to the disk cache.  Part of the back end (Sender) piece will become a
simple HitSpool client which reads data from the disk cache and sends it to
the downstream consumers, while another simple component will listen for time
intervals from the Event Builder and send back lists of hits taken from the
hit spool.

\subsubsection{Supernova System}

SN secondary stream from DOMs and SNDAQ reference.  Interface to
hitspooling.  

\subsubsection{Triggers}

General description of trigger architecture.  Separation of trigger window
and readout window.  How trigger windows depend on geometry.  Thorough
escription of all different trigger algorithms. Trigger and readout window
merging. 

% Note: JK VLVnT proceedings may be useful here

Table: standard settings for triggers

Figure: trigger windows and readout windows.

Figure: example bright multi-trigger event.  

Figure: SLOP triplet geometry?

\subsubsection{Event Building}

Readout requests to StringHub components and packaging of waveforms into
events.  Spooling to disk and interface to PnF.

%\subsubsection{The Secondary Streams (SN/Moni/TCAL)}
%Is this necessary or can we just mention these in the StringHub section?

\subsubsection{Configuration}

Tree of XML configuration files for components, triggers, and DOM settings.  

\subsubsection{Distributed Network Control}

CnC server description.  XML-RPC.  % Plus ZMQ?

\subsection{Online Filtering}
\textsl{(Erik B.; 3-4 pages)}
\subsubsection{Overview}
Big picture view of what PnF is and does (route data from DAQ output to
files for pickup by data handling, applying calibration, recos and
filters along the way, TFT content, etc)

\subsubsection{System Design}
Cover the overall system design and tech used (CORBA, I3Inlet/I3Outlets,
etc) and how all pieces fit together and are controlled (mention
pf2live). Words about historical operation in Plan B in early seasons,
plan A.

\subsubsection{Components}
A brief description of each PnF component.
\paragraph{I3DAQDispatch}
\paragraph{PnF Server}
\paragraph{PnF Clients}
\paragraph{DB Cache}
\paragraph{Realtime Followup server and clients}

\subsubsection{Performance}
\paragraph{Overall rates and known limits (server + I/O limits)}
\paragraph{Average system latency}

Figure: plot of typical latency including run transition?

\subsection{Data Handling}
\textsl{(P. Meade; 1 page)}

\textsl{Generation description of system architecture.  Stream definitions, dropboxes,
and data pickup.  Archival vs. transfer to TDRSS system.  }

Data handling is provided by three servers named jade02, jade03, and jade04. The jade servers operate independently of one another and 
each of them are capable of handling the nominal data volume by itself. Having three servers allows for data handling to continue seamlessly 
in case of hardware failure or maintenance.

Each server runs a copy of the Java Archival and Data Exchange (JADE) software (stylized “jade”). As its name implies, the jade software 
is written in the Java programming language. It is a reimplementation and expansion of earlier prototype software called South Pole Archival
and Data Exchange (SPADE), written by Cindy Mackenzie. The jade software has four primary tasks: consumption, archival, satellite transmission, and real-time
transmission.

The jade software is configured with a number of data streams, which consist of a data server, a dropbox directory, and a filename pattern. 
The data stream dropbox directories are checked on a regular basis for new files. A file pairing scheme (binary and semaphore) prevents files 
from being consumed before they are finished being produced. For each file, a checksum calculated on the data server is compared to a checksum 
calculated on the jade server. This method ensures that the file was copied without error. After this, the original data file is removed from the data host.

After consumption, files are routed according to the configuration of their data stream. Files that are too large to send via the satellite link are archived to
a configurable number of archival media copies. The prototype SPADE software archived to LTO tapes, while the later jade software archives to large (2+ TB) hard
disk drives. All of the archival data is buffered on the jade server until the archival media is complete. In case of failure while creating the archival media, 
all of the files can be immediately written to fresh archival media with a single command.

Files that are too large to send via the real-time link, but small enough to send via the satellite link are queued for satellite transmission. The jade software
attempts to bundle multiple files together into 1 GB bundle archives to allow satellite link operators to manage the daily data transmission. Very large files
(>1 GB) are split apart into multiple 1 GB bundles for the same reason. The jade software will only transfer a configurable number of bundles to the satellite
relay server. If satellite transmission is not possible, the jade software will buffer the excess bundles on the jade server, to avoid flooding the relay server
unnecessarily.


Small files (<50 KB) with high priority status information are sent via the real-time link. The real-time link is provided by the IceCube Messaging Service
(I3MS). The jade software uses JeroMQ, a pure Java implementation of the ZeroMQ (ZMQ) protocol, to connect to I3MS. In cases where the real-time link is not
available, I3MS will queue the messages to be sent when the link becomes available. All I3MS messages are also sent to jade to send via the satellite link to
ensure delivery if the real-time link should be unavailable for an extended period of time.

\subsection{IceCube-Live and Remote Monitoring}

IceCube operations are controlled and monitored centrally by IceCube-Live, a suite of high-level software implemented mostly in the Python language.
IceCube-Live consists of two major components: LiveControl, responsible for controlling data taking operations and
collecting monitoring data, and the IceCube-Live website, responsible for processing and storing monitoring data as well as presenting this data
in webpages and plots that characterize the state of the IceCube detector.

\subsubsection{LiveControl}

LiveControl is the central control component of IceCube operations.  The program executes in the background as a daemon and accepts user input
through XML-RPC.  Operators typically enter commands and check the basic detector status using a command-line interface.  LiveControl is
responsible for controlling the state of DAQ and online-processing, starting and stopping data-taking runs, and recording
the parameters of these runs.  Stardard operation is to request a run start, supplying a configuration file specifying the DOMs to include in
data taking.  LiveControl then records the run number, configuration, start time, etc. and sends a request for DAQ to
begin data taking.  After data taking commences successfully, LiveControl waits a specified amount of time, generally eight
hours, then stops the current run and automatically starts a new run using the same configuration.  This cycle continues until
stopped by a user request or a run fails.  In case of failure, LiveControl attempts to restart data taking by starting a new
run.  Occasionally a hardware failure occurs, and it is impossible to start a run with the supplied configuration because requested DOMs are
unpowered or temporarily unable to communicate with the IceCube DAQ.  In this case, LiveControl cycles through predefined partial-detector
configurations in an attempt to exclude problematic DOMs.  This results in taking data with half of the IceCube strings or fewer, but it
greatly reduces the chance of a prolonged complete outage where no data at all is recorded.

A secondary function of LiveControl is the collection, processing, and forwarding of monitoring data from DAQ, online-processing, and other components.
Monitoring data consists of a JSON dictionary with a well-definied format including a creation time, sender name, priority, data name, and either JSON data
or a single integer or floating-point value.  This data is forwarded to LiveControl using ZeroMQ and queued internally for processing.  A few monitoring
quantities indicate serious problems with the detector, e.g. the online-processing latency is too high.  LiveControl provides a database of checked monitoring
values indexed by service and data name and raises an alert if the value is out of the specified range or hasn't been received in a specified amount of time.
The alert usually includes an email to parties responsible for the affected subsystem and, for serious problems, triggers an automated page to winterover operators.
Several other types of monitoring data trigger a response by LiveControl.  These include alerts generated internally by subsystems, and such alerts may trigger emails
and pages from LiveControl.  All monitoring data are forwarded to the IceCube-Live website for further processing and display.

\subsubsection{IceCube-Live Website}

\begin{table}[!ht]
\begin{tabular}{|c|c|c|c|c|}
\hline
Priority & Transport System & Daily Messages & Daily Data & Typical Latency\\
\hline
1 & ITS & 10,000 & 1 MB & 1 minute \\
\hline
2 & I3MS & 150,000 & 5 MB & 1--5 minutes \\
\hline
3 & JADE & 300,000 & 100 MB & 1 day \\
\hline
\end{tabular}
\caption{Statistics for IceCube monitoring messages}
\label{i3messages}
\end{table}

Two operational copies of the IceCube-Live website exist: one inside the IceCube network at the South Pole, fed directly by LiveControl using ZeroMQ, and
one in the Northern Hemisphere fed by several message transport systems depending on the priority of the particular monitoring data, summarized in table
\ref{i3messages}.  
IceCube generates approximately 300,000 monitoring messages daily.  These compress to $\sim$100 MB, are sent over JADE, and are typically available
in the Northern Hemisphere within 24 hours.  Messages of priority level 2 or higher are additionally sent over the Iridium RUDICS link using I3MS,
and messages of priority 1 and higher are sent over the Iridium ITS link.

Messages reaching the website are ingested by the DBServer daemon, processed, and inserted into one of several SQL database tables or
a MongoDB database depending on content.  Messages also may contain directives requesting DBServer to send email, specifying recipients and content,
or requesting that the monitoring message be published using ZeroMQ PUB-SUB so it can be ingested by an external process.  The IceCube-Live
website itself uses the Django framework and contains pages that create sophisticated views of monitoring data stored in either SQL or MongoDB.
These pages include a front page that displays active alerts and plots of event rates and processing latencies from the previous few hours and
a page for each run that displays start time, stop time, and other essential data.  The run page contains low-level diagnostic data that
includes e.g. charge histograms, digitizer baselines, and occupancy for each DOM, and is used to diagnose problems with detector components
during the run and to determine if the run can be used in physics analysis.

Finally, the IceCube website transmits messages using ITS and I3MS to LiveControl running at the South Pole.  This capability is used to connect
the popular Slack chat service with the IceCube-Live website running at the South Pole, allowing the IceCube winterover operators to chat with
experts in the Northern Hemisphere during periods with no geosynchronous satellite connectivity.  Southbound messages are also used to trigger
the HitSpool service by the request of experts in the Northern Hemisphere if an interesting event (e.g. a SNEWS alert) occurs.

\subsection{Operational Performance}
\textsl{(John K.; 1 page)}

Explanation of how design choices, system monitoring, and winterovers result in
high uptime.  Discussion of median downtime and various causes of downtime.
Possible basic failure analysis of hardware components.  

Figure: DAQ full uptime and clean uptime percentage.

Figure (optional): Downtime histogram.  
